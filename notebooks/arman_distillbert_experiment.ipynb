{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbaf6e39-1d91-42ef-8c64-c09322daf7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "train = pd.read_csv('../data/kaggle_bias/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b23d7a3c-13d6-49fa-bf3c-1384011e6191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cfb7ca4-9b8f-494b-8dbd-08e6c9a41384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (2.17.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (4.38.1)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (0.20.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from huggingface_hub) (4.10.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81743b6e-6fd0-4784-8cd3-748df4fe736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna(subset=['comment_text'])\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "train['bi_target'] = (train['target'] >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f60fc446-306b-43cf-893b-a1b90252e8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = train.sample(frac=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f6e5e-a95c-4723-bd09-15757d94c342",
   "metadata": {},
   "source": [
    "Target class distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a38dbbe-2f3c-4abc-bb85-430fbccfc446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08006329534726009"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_sample['bi_target'].sum() / len(train_sample) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e00ee1-f04d-4b63-b188-fb52d0adc090",
   "metadata": {},
   "source": [
    "### Target Class Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ec1506a-c923-4f7c-8ef8-26fb442b56d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(train_sample, test_size=0.2, random_state=42, stratify=train_sample['bi_target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8cae1e2-98a8-4b37-8306-b7216dfc13f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72649.67798413191"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sample) * 0.08050396702273835"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d1fdad9-3ebc-4e19-9df1-2c5feb16e711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902436"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sample['bi_target'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2473fac9-ee86-47d0-a352-183a72a65c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_minority = train_df[train_df['bi_target'] == 1]\n",
    "train_sample_majority = train_df[train_df['bi_target'] == 0].sample(frac=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cda1e946-1e46-4055-9f91-919b5a2139a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_sample_minority, train_sample_majority])\n",
    "train_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "545a0f8b-4b52-4e43-a76d-52dec0cc2736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17869906634514313"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_df['bi_target'].sum() / len(train_df) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae3c0dec-0e54-47d3-a06c-9667d5b0ce6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902436"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69e832d5-f0ab-4d77-b3b0-566c6234c7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17869906634514313"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df\n",
    "(train_df['bi_target'].sum() / len(train_df) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96d5ce74-3ac1-4df9-99f3-0fc9aebad33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08006072425867648"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(val_df['bi_target'].sum() / len(val_df) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f954671a-c533-42eb-a6bf-91f8f4ca7d75",
   "metadata": {},
   "source": [
    "# -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "725beea8-615c-4b4f-9107-82918752cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataframe.iloc[idx]['comment_text']\n",
    "        label = self.dataframe.iloc[idx]['bi_target']\n",
    "        inputs = self.tokenizer(text, padding='max_length', truncation=True, max_length=215, return_tensors=\"pt\")\n",
    "        \n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "        \n",
    "        return input_ids, attention_mask, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "125b64bf-2cfa-4e1d-a36f-2f97c4f8bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# train_df, val_df = train_test_split(train_resample, test_size=0.1, random_state=42, stratify=train_resample['bi_target'])\n",
    "\n",
    "train_dataset = MyDataset(train_df, tokenizer)\n",
    "val_dataset = MyDataset(val_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc585623-731f-47b0-a35c-91f8d17952f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertConfig\n",
    "\n",
    "config = DistilBertConfig(\n",
    "    n_layers=6,  # Number of hidden layers\n",
    "    dim=768,  # Dimensionality of the encoder layers and the pooler layer\n",
    "    hidden_dim=3072,  # Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder\n",
    "    n_heads=12,  # Number of attention heads for each attention layer in the Transformer encoder\n",
    "    dropout=0.2,  # Dropout probability for the dropout layers\n",
    "    attention_dropout=0.2,  # Dropout probability for the attention layers\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', config=config)\n",
    "\n",
    "# makes the model run on the GPU instead of CPU\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cdbf89-7473-4c64-8f9f-7989471e5f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09ca0a4b9f84be6a44bd81d36c28b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/10109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e314533705ee41088edb89fe2db6df00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/10109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# class weight for 5% target class\n",
    "class_weights = [1, 1.01]\n",
    "\n",
    "# convert class weight to tensor\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).cuda()\n",
    "\n",
    "\n",
    "# AdamW optimizer is apparently really good for DistilBERT?  Will write more in docs\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Define the scheduler\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):  \n",
    "    scheduler.step()\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    for input_ids, attention_mask, labels in progress_bar:\n",
    "        # Move the training to the GPU\n",
    "        input_ids = input_ids.cuda()\n",
    "        attention_mask = attention_mask.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        # Set gradients to zero for training\n",
    "        model.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "        loss = loss_fct(outputs.logits, labels)\n",
    "\n",
    "        # loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4014ad41-0abf-45c2-87d9-e9a3922a785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "model.eval()  # Transformers built in model evaluation kit \n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for input_ids, attention_mask, labels in val_loader:\n",
    "        # Move tensors to the GPU\n",
    "        input_ids = input_ids.cuda()\n",
    "        attention_mask = attention_mask.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        # Forward pass, get logit predictions\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Move logits and labels to CPU\n",
    "        logits = outputs.logits.detach().cpu().numpy()\n",
    "        label_ids = labels.to('cpu').numpy()\n",
    "        \n",
    "        # Store predictions and true labels\n",
    "        true_labels.append(label_ids)\n",
    "        pred_labels.append(logits.argmax(axis=1))\n",
    "\n",
    "# Calculate our accuracy metrics\n",
    "true_labels = np.concatenate(true_labels)\n",
    "pred_labels = np.concatenate(pred_labels)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='binary')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77a7fe5-5f76-405a-a5a0-5922d4beb5a1",
   "metadata": {},
   "source": [
    "weight [1, 1.2] => \n",
    "Accuracy: 0.9191644966480137\n",
    "Precision: 0.4987244897959184\n",
    "Recall: 0.8072952512044047\n",
    "F1 Score: 0.616557161629435\n",
    "\n",
    "weight [1, 1.01] => \n",
    "Accuracy: 0.9252590171200621\n",
    "Precision: 0.5235934664246824\n",
    "Recall: 0.7942188575361322\n",
    "F1 Score: 0.6311184030626197"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d737c8-e657-4fa1-be9a-800bf78e3a04",
   "metadata": {},
   "source": [
    "#### First run\n",
    "5% of the data, class weights [1,19], base\n",
    "\n",
    "Accuracy: 0.8674939064923555\n",
    "\n",
    "Precision: 0.33863080684596575\n",
    "\n",
    "Recall: 0.8293413173652695\n",
    "\n",
    "F1 Score: 0.48090277777777773"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e794355c-4242-42f8-8bf5-460446bc9c2a",
   "metadata": {},
   "source": [
    "#### Second run\n",
    "2.5% of the data, class weights [1,19], scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "Accuracy: 0.8901196278245459\n",
    "\n",
    "Precision: 0.42493638676844786\n",
    "\n",
    "Recall: 0.8835978835978836\n",
    "\n",
    "F1 Score: 0.5738831615120276"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc7c1fb-f020-46aa-9afd-f1f732740ac6",
   "metadata": {},
   "source": [
    "#### Third run\n",
    "2.5% of the data, no class weights, scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "Accuracy: 0.9424014178112539\n",
    "\n",
    "Precision: 0.6710526315789473\n",
    "\n",
    "Recall: 0.5604395604395604\n",
    "\n",
    "F1 Score: 0.6107784431137724"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37829dd0-b5a9-47bc-a0b3-03f7d4c6a0c0",
   "metadata": {},
   "source": [
    "#### Fourth run\n",
    "\n",
    "random state 42 sampling, 2.5% of data, class weights [1, 11.5], scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "Accuracy: 0.8958794860434205\n",
    "\n",
    "Precision: 0.41456582633053224\n",
    "\n",
    "Recall: 0.8505747126436781\n",
    "\n",
    "F1 Score: 0.5574387947269304"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aa69c6-9dc9-4453-b59f-84bc2d66fd15",
   "metadata": {},
   "source": [
    "All above on 3 epochs --------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298994d1-5494-454c-8a36-b2e23086b953",
   "metadata": {},
   "source": [
    "Same but 5 epochs:\n",
    "\n",
    "Accuracy: 0.8887904297740363\n",
    "\n",
    "Precision: 0.41494845360824745\n",
    "\n",
    "Recall: 0.8702702702702703\n",
    "\n",
    "F1 Score: 0.5619546247818499"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cf9572-e5fa-4822-9e10-0086fb62a9fa",
   "metadata": {},
   "source": [
    "#### Fifth run\n",
    "random state 42 sampling, 2.5% of data, class weights [1, 5], scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "Accuracy: 0.9158174568010634\n",
    "\n",
    "Precision: 0.5117056856187291\n",
    "\n",
    "Recall: 0.7766497461928934\n",
    "\n",
    "F1 Score: 0.6169354838709677"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceef6c06-d383-40bb-8882-d4d192e22790",
   "metadata": {},
   "source": [
    "#### Sixth run\n",
    "random state 42 sampling, 2.5% of data, class weights [1, 2], scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "Accuracy: 0.9415152857775808\n",
    "\n",
    "Precision: 0.65\n",
    "\n",
    "Recall: 0.6770833333333334\n",
    "\n",
    "F1 Score: 0.6632653061224489"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dd6ac7-7b56-4301-8bb9-f860a7158bdc",
   "metadata": {},
   "source": [
    "#### Seventh run\n",
    "random state 42 sampling, 2.5% of data, class weights [1, 1.75], scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "Accuracy: 0.9472751439964555\n",
    "\n",
    "Precision: 0.6666666666666666\n",
    "\n",
    "Recall: 0.7204301075268817\n",
    "\n",
    "F1 Score: 0.6925064599483204"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665e82eb-e8a9-4ee9-9e34-7aeb104f0346",
   "metadata": {},
   "source": [
    "#### loss 5e-6\n",
    "\n",
    "Accuracy: 0.9295525033229951\n",
    "\n",
    "Precision: 0.6835443037974683\n",
    "\n",
    "Recall: 0.2872340425531915\n",
    "\n",
    "F1 Score: 0.40449438202247195"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a95886-0ef0-4910-af77-2b9c63216883",
   "metadata": {},
   "source": [
    "#### loss 7.5e-5\n",
    "random state 42 sampling, 2.5% of data, class weights [1, 1.7], scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "Accuracy: 0.9441736818786\n",
    "\n",
    "Precision: 0.7395833333333334\n",
    "\n",
    "Recall: 0.6513761467889908\n",
    "\n",
    "F1 Score: 0.6926829268292682"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dec369-33f7-4062-8493-78b071c582fd",
   "metadata": {},
   "source": [
    "#### Eighth run\n",
    "- random state 42 sampling, 2.5% of data, class weights [1, 2], scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "- val_set 10% of data instead of 5%\n",
    "- target class 8% in train, 8.5% in val\n",
    "    \n",
    "Accuracy: 0.941945490804343\n",
    "\n",
    "Precision: 0.6431924882629108\n",
    "\n",
    "Recall: 0.7135416666666666\n",
    "\n",
    "F1 Score: 0.6765432098765432"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a123abfb-40a4-4735-be17-2ae4fe9b06c8",
   "metadata": {},
   "source": [
    "#### Ninth run\n",
    "- random state 42 sampling, 2.5% of data, class weights [1, 1.5], scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "- val_set 10% of data instead of 5%\n",
    "- target class 8% in train, 8.5% in val\n",
    "    \n",
    "Accuracy: 0.9410591624196765\n",
    "\n",
    "Precision: 0.62882096069869\n",
    "\n",
    "Recall: 0.75\n",
    "\n",
    "F1 Score: 0.684085510688836"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f50dc6a-1ed6-47e1-a33b-5d207a33b731",
   "metadata": {},
   "source": [
    "#### Tenth run\n",
    "- random state 42 sampling, 2.5% of data, class weights [1, 3], scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "- val_set 10% of data instead of 5%\n",
    "- target class 8% in train, 8.5% in val\n",
    "    \n",
    "Accuracy: 0.9361843563040106\n",
    "\n",
    "Precision: 0.596\n",
    "\n",
    "Recall: 0.7760416666666666\n",
    "\n",
    "F1 Score: 0.67420814479638"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24860234-1b5e-4a6a-9ac7-efb1c7b2d9b9",
   "metadata": {},
   "source": [
    "#### Eleventh run\n",
    "- random state 42 sampling, 2.5% of data, class weights [1, 2], scheduler = StepLR(optimizer, step_size=1, gamma=0.1), loss = 9e-5\n",
    "- val_set 10% of data instead of 5%\n",
    "- target class 8% in train, 8.5% in val\n",
    "    \n",
    "Accuracy: 0.9454908043430091\n",
    "\n",
    "Precision: 0.6691176470588235\n",
    "\n",
    "Recall: 0.7109375\n",
    "\n",
    "F1 Score: 0.6893939393939394"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab354245-8116-49ce-aeca-6cd5fecbe463",
   "metadata": {},
   "source": [
    "#### Twelveth run\n",
    "- random state 42 sampling, 2.5% of data, class weights [1, 1.7], scheduler = StepLR(optimizer, step_size=1, gamma=0.1), loss = 7.5e-5\n",
    "- val_set 10% of data instead of 5%\n",
    "- target class 8% in train, 8.5% in val\n",
    "    \n",
    "Accuracy: 0.9459339685353424\n",
    "\n",
    "Precision: 0.6741293532338308\n",
    "\n",
    "Recall: 0.7057291666666666\n",
    "\n",
    "F1 Score: 0.6895674300254454"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93867a8c-05cb-4e46-a1fd-b078744b4f6e",
   "metadata": {},
   "source": [
    "### Undersampling Majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1b81b1-a62a-4d84-bed9-0060f29799c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
