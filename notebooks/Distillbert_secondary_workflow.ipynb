{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b990f513-eaf8-4cbc-bff9-ebdf122a3534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b173887c-de0f-4c5b-8d39-20f696c9a90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>bi_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  bi_target\n",
       "0  This is so cool. It's like, 'would you want yo...          0\n",
       "1  Thank you!! This would make my life a lot less...          0\n",
       "2  This is such an urgent design problem; kudos t...          0\n",
       "3  Is this something I'll be able to install on m...          0\n",
       "4               haha you guys are a bunch of losers.          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../data/kaggle_bias/train.csv')\n",
    "\n",
    "train = train.dropna(subset=['comment_text'])\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "train['bi_target'] = (train['target'] >= 0.5).astype(int)\n",
    "\n",
    "df = train[['comment_text', 'bi_target']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a164df58-cd39-45c5-a02f-d453e4d73988",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(frac=0.5, random_state=42)\n",
    "train_df, test_df = train_test_split(df_sample, test_size=0.2, random_state=42, stratify=df_sample['bi_target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0e0d093-aafc-4a3e-a69f-bb49810ee4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "721948"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e057a1-31a1-4c65-a761-557ad344a8d8",
   "metadata": {},
   "source": [
    "### Undersampling Majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6d492a2-cbe8-4696-927e-1944acde0a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1304349788332566"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rus = RandomUnderSampler(sampling_strategy=0.15, random_state=42)\n",
    "train_df_rus, _ = rus.fit_resample(train_df, train_df['bi_target'])\n",
    "(train_df_rus['bi_target'].sum() / len(train_df_rus) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ee1aa25-44fb-488e-9b35-73aacb001f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443148"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2fdc0c6-be0d-403a-99c9-d505b2d04d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bi_target\n",
       "0    385346\n",
       "1     57802\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_rus['bi_target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58613864-ce22-4c79-b24c-cca65e5d099d",
   "metadata": {},
   "source": [
    "### Oversampling Minority Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a9acea9-7380-4214-876a-ecce6e559dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy requests nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94a91c83-84a7-42c8-a5f3-3b1e1d5b42a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as nlpaw\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "053ffaa2-fec5-4942-8cc3-28da38b2bdb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'nlpaug.augmenter.word' from 'C:\\\\Users\\\\arman\\\\anaconda3\\\\envs\\\\torchpower\\\\lib\\\\site-packages\\\\nlpaug\\\\augmenter\\\\word\\\\__init__.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlpaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "287c4c92-3743-48b5-ba5c-0baf803c4975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nlpaug.augmenter.word as nlpaw\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "495fa94b-5814-4235-94ac-c2f7a1359168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [5:52:52<00:00, 7057.64s/it]  \n"
     ]
    }
   ],
   "source": [
    "def augment_sentence(sentence, aug, num_threads):\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    Constructs a new sentence via text augmentation.\n",
    "    \n",
    "    Input:\n",
    "        - sentence:     A string of text\n",
    "        - aug:          An augmentation object defined by the nlpaug library\n",
    "        - num_threads:  Integer controlling the number of threads to use if\n",
    "                        augmenting text via CPU\n",
    "    Output:\n",
    "        - A string of text that been augmented\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    return aug.augment(sentence, num_thread=num_threads)\n",
    "    \n",
    "\n",
    "\n",
    "def augment_text(df, aug, num_threads, num_times):\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    Takes a pandas DataFrame and augments its text data.\n",
    "    \n",
    "    Input:\n",
    "        - df:            A pandas DataFrame containing the columns:\n",
    "                                - 'comment_text' containing strings of text to augment.\n",
    "                                - 'bi_target' binary target variable containing 0's and 1's.\n",
    "        - aug:           Augmentation object defined by the nlpaug library.\n",
    "        - num_threads:   Integer controlling number of threads to use if augmenting\n",
    "                         text via CPU\n",
    "        - num_times:     Integer representing the number of times to augment text.\n",
    "    Output:\n",
    "        - df:            Copy of the same pandas DataFrame with augmented data \n",
    "                         appended to it and with rows randomly shuffled.\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    \n",
    "    # Get rows of data to augment\n",
    "    to_augment = df[df['bi_target']==1]\n",
    "    to_augmentX = to_augment['comment_text']\n",
    "    to_augmentY = np.ones(len(to_augmentX.index) * num_times, dtype=np.int8)\n",
    "    \n",
    "    # Build up dictionary containing augmented data\n",
    "    aug_dict = {'comment_text':[], 'bi_target':to_augmentY}\n",
    "    for i in tqdm(range(num_times)):\n",
    "        augX = [augment_sentence(x, aug, num_threads) for x in to_augmentX]\n",
    "        aug_dict['comment_text'].extend(augX)\n",
    "    \n",
    "    # Build DataFrame containing augmented data\n",
    "    aug_df = pd.DataFrame.from_dict(aug_dict)\n",
    "    \n",
    "    return pd.concat([df, aug_df], ignore_index=True).sample(frac=1, random_state=42)\n",
    "    \n",
    "\n",
    "    \n",
    "# Define nlpaug augmentation object \n",
    "aug10p = nlpaw.ContextualWordEmbsAug(model_path='bert-base-uncased', aug_min=1, aug_p=0.026, action=\"substitute\")\n",
    "\n",
    "# Upsample minority class ('bi_target' == 1) to create a roughly 50-50 class distribution\n",
    "balanced_df = augment_text(train_df_rus, aug10p, num_threads=8, num_times=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1873aca9-5363-4f73-868f-c7fc86e335d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>bi_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95478</th>\n",
       "      <td>How does Oasis differ from Unitarians?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194807</th>\n",
       "      <td>Nope!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7622</th>\n",
       "      <td>As I recall Fr Martin tweeted out that those t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107145</th>\n",
       "      <td>Muckler was a bad GM...Brian Murray is a real ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65899</th>\n",
       "      <td>This \"post their names and faces all over soci...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  bi_target\n",
       "95478              How does Oasis differ from Unitarians?          0\n",
       "194807                                              Nope!          0\n",
       "7622    As I recall Fr Martin tweeted out that those t...          0\n",
       "107145  Muckler was a bad GM...Brian Murray is a real ...          0\n",
       "65899   This \"post their names and faces all over soci...          0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f30588ab-515b-4aec-a6b6-c0c6bfec1418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3750004054794876"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(balanced_df['bi_target'].sum() / len(balanced_df) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b14f680-a215-4115-9d65-4a034451fcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "616554"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(balanced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b012937-caf7-4e4e-85ae-9cc062af6b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "597e5fa2-c0e6-4681-b23d-c36ca7109065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataframe.iloc[idx]['comment_text']\n",
    "        label = self.dataframe.iloc[idx]['bi_target']\n",
    "        inputs = self.tokenizer(text, padding='max_length', truncation=True, max_length=215, return_tensors=\"pt\")\n",
    "        \n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "        \n",
    "        return input_ids, attention_mask, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "754297da-a6a9-4411-9d40-a0f58ce26c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_dataset = MyDataset(balanced_df, tokenizer)\n",
    "val_dataset = MyDataset(test_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8eb2957-27d2-4d28-a600-aca77c45da43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertConfig\n",
    "\n",
    "config = DistilBertConfig(\n",
    "    n_layers=6,  # Number of hidden layers\n",
    "    dim=768,  # Dimensionality of the encoder layers and the pooler layer\n",
    "    hidden_dim=3072,  # Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder\n",
    "    n_heads=12,  # Number of attention heads for each attention layer in the Transformer encoder\n",
    "    dropout=0.2,  # Dropout probability for the dropout layers\n",
    "    attention_dropout=0.2,  # Dropout probability for the attention layers\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', config=config)\n",
    "\n",
    "# makes the model run on the GPU instead of CPU\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d4691d4-57ff-4562-abb6-dd07455d9d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arman\\anaconda3\\envs\\torchpower\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd53ebdc7324fc79c6c02b3cdfc1988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/19268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c978d3313564bfb8da818c1bb8ffdfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/19268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e0d121fcb24410861df3f67df22c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/19268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# # class weight for 5% target class\n",
    "# class_weights = [1, 1.01]\n",
    "\n",
    "# # convert class weight to tensor\n",
    "# class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).cuda()\n",
    "\n",
    "\n",
    "# AdamW optimizer is apparently really good for DistilBERT?  Will write more in docs\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Define the scheduler\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):  \n",
    "    scheduler.step()\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    for input_ids, attention_mask, labels in progress_bar:\n",
    "        # Move the training to the GPU\n",
    "        input_ids = input_ids.cuda()\n",
    "        attention_mask = attention_mask.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        # Set gradients to zero for training\n",
    "        model.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        # loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "        # loss = loss_fct(outputs.logits, labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26333646-70f8-47b7-98c4-c331351fae41",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Apply sigmoid to logits\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     24\u001b[0m pred_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(torch\u001b[38;5;241m.\u001b[39mtensor(logits))\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Move labels to CPU\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "model.eval()  # Transformers built-in model evaluation kit \n",
    "true_labels = np.array([])\n",
    "all_pred_labels = []  # Store predictions for each threshold\n",
    "\n",
    "# Define a list of threshold values to iterate over\n",
    "thresholds = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "for threshold in thresholds:\n",
    "    true_labels = np.array([])\n",
    "    pred_labels = np.array([])\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for input_ids, attention_mask, labels in val_loader:\n",
    "            # Move tensors to the GPU\n",
    "            input_ids = input_ids.cuda()\n",
    "            attention_mask = attention_mask.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            # Forward pass, get logit predictions\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Apply threshold to logits\n",
    "            logits = outputs.logits.detach().cpu().numpy()\n",
    "            pred_probs = torch.softmax(torch.tensor(logits), dim=1)\n",
    "            pred_labels_thresholded = (pred_probs[:, 1] > threshold).numpy()  # Assuming binary classification\n",
    "\n",
    "            # Move labels to CPU\n",
    "            label_ids = labels.to('cpu').numpy()\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            true_labels = np.concatenate((true_labels, label_ids))\n",
    "            pred_labels = np.concatenate((pred_labels, pred_labels_thresholded))\n",
    "\n",
    "    all_pred_labels.append(pred_labels)\n",
    "\n",
    "# Convert list of prediction arrays to numpy array\n",
    "all_pred_labels = np.array(all_pred_labels)\n",
    "\n",
    "# Calculate metrics for each threshold\n",
    "for i, threshold in enumerate(thresholds):\n",
    "    pred_labels = all_pred_labels[i]\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='binary')\n",
    "\n",
    "    print(f'Threshold: {threshold}')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print('///////////////////////')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03c0f02f-ee34-40d0-a452-f788714b67e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = balanced_df['comment_text']\n",
    "# y_train = balanced_df['bi_target']\n",
    "# X_test = test_df['comment_text']\n",
    "# y_test = test_df['bi_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f7b60f-1840-4b42-891c-064d9d7be7df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
